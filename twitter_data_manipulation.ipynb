{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Codice per la manipolazione di dati estratti dalle API v2 di Twitter\n",
    "\n",
    "## Funzionalit√†\n",
    "\n",
    "##### Estrazione di risposte a tweet\n",
    "##### Estrazione di interazione utente-utente con salvataggio di timestamp e testo del tweet\n",
    "##### Estrazione di tweet a partire da nome utente con l'uso di Twint (https://github.com/twintproject/twint)\n",
    "##### Creazione di csv con i dati elaborati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "import numpy as np\n",
    "import ijson\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "import glob\n",
    "import json\n",
    "import csv\n",
    "from NormalizeTweets import normalizeTweet\n",
    "\n",
    "# generazione embedding\n",
    "import gensim\n",
    "import gensim.downloader as api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path dei dati\n",
    "singleline_json = \"C:\\\\Users\\\\federico\\\\Documents\\\\MEGA\\\\UNI\\\\2-ML\\\\Progetto\\\\ijson\\\\singleline.json\"\n",
    "table_json = \"C:\\\\Users\\\\federico\\\\Documents\\\\MEGA\\\\UNI\\\\2-ML\\\\Progetto\\\\results.json\"\n",
    "table_json_nb = \"D:\\\\Progetto\\\\qatar150k2411.json\"\n",
    "table_json_results = \"results.json\"\n",
    "table_json_nb_70k = \"D:\\\\Progetto\\\\qatar70k1911.json\"\n",
    "table_json_theirs = \"D:\\\\row_datsets\\\\theirs0401-1001.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conversione data in stringa -> timestamp\n",
    "def dateToTimestampFromTwitter(date):\n",
    "    dt = datetime.strptime(date, '%Y-%m-%dT%H:%M:%S.%fZ')\n",
    "    return time.mktime(dt.timetuple())\n",
    "\n",
    "\n",
    "def dateToTimestamp(date):\n",
    "    dt = datetime.strptime(date, '%Y-%m-%d %H:%M:%S')\n",
    "    return time.mktime(dt.timetuple())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modello doc2vec per la generazione degli embedding\n",
    "dataset = api.load(\"text8\")\n",
    "data = [d for d in dataset]\n",
    "\n",
    "def tagged_document(list_of_list_of_words):\n",
    "   for i, list_of_words in enumerate(list_of_list_of_words):\n",
    "      yield gensim.models.doc2vec.TaggedDocument(list_of_words, [i])\n",
    "data_training = list(tagged_document(data))\n",
    "\n",
    "model = gensim.models.doc2vec.Doc2Vec(vector_size=768, min_count=2, epochs=30)\n",
    "\n",
    "model.build_vocab(data_training)\n",
    "\n",
    "# training del modello\n",
    "model.train(data_training, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funzione per la generazione degli embedding a partire dal testo (array di parole)\n",
    "def getDoc2VecEmbeddingGensim(text):\n",
    "    return model.infer_vector(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternativa al doc2vec: glove-twitter-200, glove-twitter-100, ... ma produce embedding di dimensione fissa\n",
    "\n",
    "#modelGloveTwitter = api.load(\"glove-twitter-25\")\n",
    "\n",
    "#def getEmbeddingGloveTwitterGensim(text):\n",
    "#    return modelGloveTwitter.get_mean_vector(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creazione del dataframe contenente le informazioni necessarie per l'elaborazione a partire dai dati estratti da twitter\n",
    "\n",
    "tweetCount = 0\n",
    "df = pd.DataFrame(columns=[\"author_id\", \"tweet_id\", \"conversation_id\", \"timestamp\", \"text\"])\n",
    "\n",
    "with open(table_json_theirs, 'rb') as input_file:\n",
    "    parser = ijson.parse(input_file, multiple_values=True)\n",
    "    \n",
    "    curr_author = 0\n",
    "    curr_tweet = 0\n",
    "    curr_text = ''\n",
    "    curr_date = ''\n",
    "    curr_conversation = 0\n",
    "    \n",
    "    curr_user_id = 0   \n",
    "    curr_username = ''\n",
    "\n",
    "    conversation2tweets = {}\n",
    "    tweet2author = {}\n",
    "    tweet2text = {}\n",
    "    tweet2date = {}\n",
    "    tweetIds = set()\n",
    "    authors = set()\n",
    "       \n",
    "    for prefix, event, value in parser:\n",
    "        if prefix==\"includes.tweets.item.text\":# or prefix==\"data.item.text\":\n",
    "            if curr_text != '' and curr_tweet!=0:\n",
    "                df.loc[len(df.index)] = [curr_author, curr_tweet, curr_conversation, curr_date, curr_text]\n",
    "            curr_text = value\n",
    "        if prefix==\"includes.tweets.item.author_id\":# or prefix==\"data.item.author_id\":\n",
    "            curr_author = value\n",
    "            authors.add(value)\n",
    "        if prefix==\"includes.tweets.item.id\":# or prefix==\"data.item.id\":\n",
    "            #tweetIds.add(value)\n",
    "            tweetCount += 1\n",
    "            curr_tweet = value\n",
    "            tweet2author[value] = curr_author\n",
    "            tweet2text[value] = curr_text\n",
    "        if prefix==\"includes.tweets.item.conversation_id\":# or prefix==\"data.item.conversation_id\":\n",
    "            #tweetIds.add(value)\n",
    "            curr_conversation = value\n",
    "            \"\"\"if value != curr_tweet:\n",
    "                if conversation2tweets.get(value) == None:\n",
    "                    values = []\n",
    "                    values.append(curr_tweet)\n",
    "                    conversation2tweets[value] = values\n",
    "                else:\n",
    "                    values = conversation2tweets.get(value)\n",
    "                    values.append(curr_tweet)\"\"\"\n",
    "        if prefix==\"includes.tweets.item.created_at\":# or prefix==\"data.item.created_at\":\n",
    "            curr_date = dateToTimestampFromTwitter(value)\n",
    "            tweet2date[curr_tweet] = curr_date\n",
    "\n",
    "df.to_json('df_withauthorid.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# salvataggio delle corrispondenze user_id <-> username per non effettuare altre chiamate per recuperare questa informazione\n",
    "\n",
    "df_users = pd.DataFrame(columns=[\"user_id\", \"username\"])\n",
    "with open(table_json_nb, 'rb') as input_file:\n",
    "    parser = ijson.parse(input_file, multiple_values=True)\n",
    "\n",
    "    curr_id = 0   \n",
    "    curr_username = ''\n",
    "    \n",
    "    for prefix, event, value in parser:\n",
    "        if prefix==\"data.item.entities.mentions.item.username\":# or prefix==\"data.item.text\":\n",
    "            if curr_id != 0 and curr_username !='':\n",
    "                df_users.loc[len(df.index)] = [curr_id, curr_username]\n",
    "            curr_username = value\n",
    "        if prefix==\"data.item.entities.mentions.item.id\":# or prefix==\"data.item.text\":\n",
    "            curr_id = value\n",
    "        \n",
    "\n",
    "df_users.to_json('df_authors_username.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generazione degli indici da salvare nel dataset finale e salvataggio corrispondenza user_index <-> user_id\n",
    "count = 0\n",
    "user_idxs = {}\n",
    "for user_id in df['author_id']:\n",
    "    user_idxs[user_id] = count\n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generazione degli indici per i tweet per ricavare l'autore nel dataset finale e salvataggio corrispondenza tweet_index <-> tweet_id\n",
    "cnt = 0\n",
    "tweets_ids = {}\n",
    "for tweet_id in df['tweet_id']:\n",
    "    tweets_ids[tweet_id] = cnt\n",
    "    cnt+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generazione lista dei tweet ricavati per filtrare quelli di cui non si hanno abbastanza informazioni\n",
    "tweets_ids_list = []\n",
    "for tweet_id in df['tweet_id']:\n",
    "    tweets_ids_list.append(tweet_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtraggio\n",
    "df_filtered = df[df['conversation_id'].isin(tweets_ids_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corrispondenza tweet <-> autore\n",
    "tweet2author = {}\n",
    "c = 0\n",
    "for user_id, tweet_id in zip(df_filtered['user_id'], df_filtered['tweet_id']):\n",
    "    tweet2author[tweet_id] = user_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# salvataggio del dataset finale contenente id_autore_tweet_risposta, id_autore_tweet_iniziale, timestamp_tweet, testo_tweet\n",
    "df_final = pd.DataFrame(columns=[\"user_id\", \"item_id\", \"timestamp\", \"text\"])\n",
    "errors = 0\n",
    "for index, row in df_filtered.iterrows():\n",
    "    try:\n",
    "        df_final.loc[len(df.index)] = [row['user_id'], tweet2author[row['conversation_id']], time.mktime(row['timestamp'].timetuple()), row['text']]\n",
    "    except:\n",
    "        errors+=1\n",
    "print(str(errors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# salvataggio df per estrazione statistiche\n",
    "df_final.to_json('df_final.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ordinamento tweet per timestamp, normalizzazione valori timestamp e salvataggio dataset in csv (con normalizzazione del testo)\n",
    "df_final.sort_values(by='timestamp')\n",
    "df_final['timestamp'] = df_final['timestamp'] - df_final['timestamp'].min()\n",
    "\n",
    "start = time.time()\n",
    "with open('D:\\dataset_input.csv', 'w') as csvfile:\n",
    "    filewriter = csv.writer(csvfile, delimiter=',',\n",
    "                            quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "    filewriter.writerow(['user_id','item_id','timestamp', 'state_label','comma_separated_list_of_features'])\n",
    "    for index, row in df_final.iterrows():\n",
    "        try:\n",
    "            filewriter.writerow([row.user_id, row.item_id, row.timestamp, 0, *getDoc2VecEmbeddingGensim(normalizeTweet(row.text).split(\" \"))])\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "end = time.time()\n",
    "print(\"The time of execution of above program is :\", (end-start) * 10**3, \"ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rimozione righe vuote\n",
    "# rimozione righe vuote\n",
    "with open('D:\\dataset_input.csv') as input, open('tgn/data/dataset_input.csv', 'w', newline='') as output:\n",
    "    writer = csv.writer(output)\n",
    "    for row in csv.reader(input):\n",
    "        if any(field.strip() for field in row):\n",
    "            writer.writerow(row)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SCRAPING PER UTENTE\n",
    "### Per questa fase √® utile memorizzare la corrispondenza indice utente <-> username\n",
    "###### Con le funzioni usate in questo notebook, √® possibile ricavare lo username scorrendo la lista degli user id nel dataframe e controllando la corrispondenza nel dataframe authors_username. √® poi possibile generare di nuovo gli indici e ricavare il corrispondente indice (questo √® un WORKAROUND che si pu√≤ evitare salvando direttamente le corrispondenze indice<->id<->username)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import twint\n",
    "\n",
    "#read df with author_ids\n",
    "df_raw = pd.read_json('df_withauthorid.json')\n",
    "\n",
    "#df with author_id to username correspondance\n",
    "df_users = pd.read_json(\"df_authors_username.json\")\n",
    "#delete duplicates\n",
    "df_users = df_users.drop_duplicates()\n",
    "\n",
    "#get users' indexes used in the final dataframe\n",
    "count = 0\n",
    "user_idxs = {}\n",
    "for user_id in df_raw['author_id']:\n",
    "    user_idxs[user_id] = count\n",
    "    count+=1\n",
    "\n",
    "errors = 0\n",
    "counter = 0\n",
    "start = time.time()\n",
    "\n",
    "for author_id in df_raw['author_id'].drop_duplicates():\n",
    "    try:\n",
    "        # estrazione username\n",
    "        username = df_users[df_users['user_id']==author_id].drop_duplicates()['username'].values[0]\n",
    "        # creazione oggetto twint per lo scraping\n",
    "        c = twint.Config()\n",
    "        c.Username = username\n",
    "        c.Limit = 100 # estrazione di 100 tweet per utente\n",
    "        c.Output = \"tweets/tweets-\"+str(user_idxs[author_id])+\".json\" # salvataggio dei tweet in un file json con nome contenente l'indice dell'utente\n",
    "        c.Store_json = True\n",
    "        twint.run.Search(c)\n",
    "        counter+=1\n",
    "        print(username + \"saved\") # log degli utenti salvati\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        errors +=1\n",
    "\n",
    "print(\"Time elapsed: \"+ str(time.time() - start))\n",
    "print(\"Saved:\" + str(counter))\n",
    "print(\"Errors:\" + str(errors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading completed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_idx</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100092</td>\n",
       "      <td>üá™üá®‚ùåÔ∏è‚öΩÔ∏è ¬°ECUADOR NO CONOCE LA VICTORIA EN EL H...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100419</td>\n",
       "      <td>@VoltInuOfficial $VOLT ü•áüåç‚ö°‚ö°‚ö°‚ö°‚ö°‚ö°‚ö°‚ö°  https://t....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100511</td>\n",
       "      <td>@aghachurchill @unrulified ÿ≥ŸÑÿßŸÖ ÿÆ€åŸÑ€å ÿÆŸàÿ® ⁄©ÿßÿ± ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10065</td>\n",
       "      <td>Eat it‚Ä¶ Feel it‚Ä¶ Love it‚Ä¶  Celebrate everyday...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100709</td>\n",
       "      <td>¬°Lindo inicio de semana para todos!   #FelizL...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3102</th>\n",
       "      <td>99833</td>\n",
       "      <td>U.S. charges neo-Nazi leader over plot to att...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3103</th>\n",
       "      <td>99834</td>\n",
       "      <td>#LunesDeGanarSeguidores #mexicana #sexy  http...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3104</th>\n",
       "      <td>9991</td>\n",
       "      <td>Your gauge determines your week!  Choose wise...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3105</th>\n",
       "      <td>99938</td>\n",
       "      <td>30 anggota @InterClubIndo merayakan kemenanga...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3106</th>\n",
       "      <td>99965</td>\n",
       "      <td>‡Æö‡Æ∞‡Øç‡Æµ‡Æ§‡Øá‡Æö ‡Æï‡Æø‡Æ∞‡Æø‡Æï‡Øç‡Æï‡ØÜ‡Æü‡Øç ‡Æ™‡Øã‡Æü‡Øç‡Æü‡Æø‡Æï‡Æ≥‡Æø‡Æ≤‡Øç ‡Æá‡Æ∞‡ØÅ‡Æ®‡Øç‡Æ§‡ØÅ ‡Æì‡ÆØ‡Øç‡Æµ‡ØÅ ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3107 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     user_idx                                               text\n",
       "0      100092   üá™üá®‚ùåÔ∏è‚öΩÔ∏è ¬°ECUADOR NO CONOCE LA VICTORIA EN EL H...\n",
       "1      100419   @VoltInuOfficial $VOLT ü•áüåç‚ö°‚ö°‚ö°‚ö°‚ö°‚ö°‚ö°‚ö°  https://t....\n",
       "2      100511   @aghachurchill @unrulified ÿ≥ŸÑÿßŸÖ ÿÆ€åŸÑ€å ÿÆŸàÿ® ⁄©ÿßÿ± ...\n",
       "3       10065   Eat it‚Ä¶ Feel it‚Ä¶ Love it‚Ä¶  Celebrate everyday...\n",
       "4      100709   ¬°Lindo inicio de semana para todos!   #FelizL...\n",
       "...       ...                                                ...\n",
       "3102    99833   U.S. charges neo-Nazi leader over plot to att...\n",
       "3103    99834   #LunesDeGanarSeguidores #mexicana #sexy  http...\n",
       "3104     9991   Your gauge determines your week!  Choose wise...\n",
       "3105    99938   30 anggota @InterClubIndo merayakan kemenanga...\n",
       "3106    99965   ‡Æö‡Æ∞‡Øç‡Æµ‡Æ§‡Øá‡Æö ‡Æï‡Æø‡Æ∞‡Æø‡Æï‡Øç‡Æï‡ØÜ‡Æü‡Øç ‡Æ™‡Øã‡Æü‡Øç‡Æü‡Æø‡Æï‡Æ≥‡Æø‡Æ≤‡Øç ‡Æá‡Æ∞‡ØÅ‡Æ®‡Øç‡Æ§‡ØÅ ‡Æì‡ÆØ‡Øç‡Æµ‡ØÅ ...\n",
       "\n",
       "[3107 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "count=0\n",
    "df_user2text = pd.DataFrame(columns=('user_idx', 'text'))\n",
    "files = glob.glob('tweets/*', recursive=True)\n",
    "# per ogni utente, i tweet vengono concatenati e salvati in un dataframe\n",
    "for single_file in files:\n",
    "    with open(single_file, 'r') as f:\n",
    "        user_idx = str(single_file).split('-')[1].split('.')[0]\n",
    "        parser = ijson.parse(single_file, multiple_values=True, )      \n",
    "        tweet = ''\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            tweet = tweet + ' ' + data[u'tweet']\n",
    "        df_user2text.loc[len(df_user2text)] = [user_idx, tweet]\n",
    "        \n",
    "# per ogni utente si genera l'embedding del testo complessivo e si salva nel dataset, assieme all'indice dell'utente\n",
    "with open('D:\\dataset_utenti.csv', 'w') as csvfile:\n",
    "    filewriter = csv.writer(csvfile, delimiter=',',\n",
    "                            quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "    filewriter.writerow(['user_id', 'comma_separated_list_of_features'])\n",
    "    for index, row in df_user2text.iterrows():\n",
    "        try:\n",
    "            # √® necessario che ogni vettore di embedding sia composto da 172 valori (√® necessario allenare un nuovo modello doc2vec: model = gensim.models.doc2vec.Doc2Vec(vector_size=172, min_count=2, epochs=30) )\n",
    "            filewriter.writerow([row.user_idx, *getDoc2VecEmbeddingGensim(normalizeTweet(row.text).split(\" \"))])\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rimozione righe vuote e salvataggio dataset nel path data (il nome deve essere '{dataName}_nodes.csv')\n",
    "with open('D:\\dataset_utenti.csv') as input, open('tgn/data/dataset_input_nodes.csv', 'w', newline='') as output:\n",
    "    writer = csv.writer(output)\n",
    "    for row in csv.reader(input):\n",
    "        if any(field.strip() for field in row):\n",
    "            writer.writerow(row)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistiche sui dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('df_final.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stats = pd.DataFrame(columns=[\"user_id\", \"interactions\"])\n",
    "df_stats_asCreator = pd.DataFrame(columns=[\"user_id\", \"interactions\"])\n",
    "df_stats_asCreator['user_id'] = df[df['user_id']!=df['item_id']]['user_id'].value_counts().index\n",
    "df_stats_asCreator['interactions'] = df[df['user_id']!=df['item_id']]['user_id'].value_counts().values\n",
    "df_stats_asReplyer = pd.DataFrame(columns=[\"user_id\", \"interactions\"])\n",
    "df_stats_asReplyer['user_id'] = df[df['user_id']!=df['item_id']]['item_id'].value_counts().index\n",
    "df_stats_asReplyer['interactions'] = df[df['user_id']!=df['item_id']]['item_id'].value_counts().values\n",
    "\n",
    "df_stats = pd.concat([df_stats_asCreator, df_stats_asReplyer])\\\n",
    "       .groupby('user_id')['interactions']\\\n",
    "       .sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stats['interactions'].plot(kind='kde', color=\"darkblue\", xlabel=\"#interactions\",figsize=(20,5))\n",
    "plt.grid(color='blue', linestyle='-', linewidth=0.5, axis=\"both\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Statistiche numero di interazioni')\n",
    "print('Minimo: ' + str(df_stats['interactions'].min()))\n",
    "print('Moda: ' + str(df_stats['interactions'].mode()))\n",
    "print('Mediana: ' + str(df_stats['interactions'].median()))\n",
    "print('Media: ' + str(df_stats['interactions'].mean()))\n",
    "print('Massimo: ' + str(df_stats['interactions'].max()))\n",
    "print('Totale: ' + str(df_stats['interactions'].sum()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6e507f021528ee598d4189b61a0c3eda1bdaecd993998d2871a13eaf733316a7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
