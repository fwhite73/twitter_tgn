{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Codice per la manipolazione di dati estratti dalle API v2 di Twitter\n",
    "\n",
    "## Funzionalit√†\n",
    "\n",
    "##### Estrazione di risposte a tweet\n",
    "##### Estrazione di interazione utente-utente con salvataggio di timestamp e testo del tweet\n",
    "##### Estrazione di tweet a partire da nome utente con l'uso di Twint (https://github.com/twintproject/twint)\n",
    "##### Creazione di csv con i dati elaborati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "import numpy as np\n",
    "import ijson\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "import glob\n",
    "import json\n",
    "import csv\n",
    "from NormalizeTweets import normalizeTweet\n",
    "\n",
    "# generazione embedding\n",
    "import gensim\n",
    "import gensim.downloader as api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\feder\\anaconda3\\envs\\bert\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "# from transformers import TFAutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "import csv\n",
    "import urllib.request\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "task='sentiment'\n",
    "MODEL = f\"cardiffnlp/twitter-roberta-base-{task}\"\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "\n",
    "# # download label mapping\n",
    "labels=[]\n",
    "mapping_link = f\"https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/{task}/mapping.txt\"\n",
    "with urllib.request.urlopen(mapping_link) as f:\n",
    "    html = f.read().decode('utf-8').split(\"\\n\")\n",
    "    csvreader = csv.reader(html, delimiter='\\t')\n",
    "labels = [row[1] for row in csvreader if len(row) > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path dei dati\n",
    "singleline_json = \"C:\\\\Users\\\\federico\\\\Documents\\\\MEGA\\\\UNI\\\\2-ML\\\\Progetto\\\\ijson\\\\singleline.json\"\n",
    "table_qatar = \"data/qatar.json\"\n",
    "table_json_nb = \"D:\\\\Progetto\\\\qatar150k2411.json\"\n",
    "table_json_results = \"results.json\"\n",
    "table_json_nb_70k = \"D:\\\\Progetto\\\\qatar70k1911.json\"\n",
    "table_json_theirs = \"D:\\\\row_datsets\\\\theirs0401-1001.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conversione data in stringa -> timestamp\n",
    "def dateToTimestampFromTwitter(date):\n",
    "    dt = datetime.strptime(date, '%Y-%m-%dT%H:%M:%S.%fZ')\n",
    "    return time.mktime(dt.timetuple())\n",
    "\n",
    "\n",
    "def dateToTimestamp(date):\n",
    "    dt = datetime.strptime(date, '%Y-%m-%d %H:%M:%S')\n",
    "    return time.mktime(dt.timetuple())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modello doc2vec per la generazione degli embedding\n",
    "dataset = api.load(\"text8\")\n",
    "data = [d for d in dataset]\n",
    "\n",
    "def tagged_document(list_of_list_of_words):\n",
    "   for i, list_of_words in enumerate(list_of_list_of_words):\n",
    "      yield gensim.models.doc2vec.TaggedDocument(list_of_words, [i])\n",
    "data_training = list(tagged_document(data))\n",
    "\n",
    "model = gensim.models.doc2vec.Doc2Vec(vector_size=768, min_count=2, epochs=30)\n",
    "\n",
    "model.build_vocab(data_training)\n",
    "\n",
    "# training del modello\n",
    "model.train(data_training, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funzione per la generazione degli embedding a partire dal testo (array di parole)\n",
    "def getDoc2VecEmbeddingGensim(text):\n",
    "    return model.infer_vector(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternativa al doc2vec: glove-twitter-200, glove-twitter-100, ... ma produce embedding di dimensione fissa\n",
    "\n",
    "#modelGloveTwitter = api.load(\"glove-twitter-25\")\n",
    "\n",
    "#def getEmbeddingGloveTwitterGensim(text):\n",
    "#    return modelGloveTwitter.get_mean_vector(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creazione del dataframe contenente le informazioni necessarie per l'elaborazione a partire dai dati estratti da twitter\n",
    "\n",
    "tweetCount = 0\n",
    "df = pd.DataFrame(columns=[\"author_id\", \"tweet_id\", \"conversation_id\", \"timestamp\", \"text\", \"sentiment\"])\n",
    "\n",
    "with open(table_qatar, 'rb') as input_file:\n",
    "    parser = ijson.parse(input_file, multiple_values=True)\n",
    "    \n",
    "    curr_author = 0\n",
    "    curr_tweet = 0\n",
    "    curr_text = ''\n",
    "    curr_date = ''\n",
    "    curr_conversation = 0\n",
    "    \n",
    "    curr_user_id = 0   \n",
    "    curr_username = ''\n",
    "\n",
    "    conversation2tweets = {}\n",
    "    tweet2author = {}\n",
    "    tweet2text = {}\n",
    "    tweet2date = {}\n",
    "    tweetIds = set()\n",
    "    authors = set()\n",
    "       \n",
    "    for prefix, event, value in parser:\n",
    "        if prefix==\"includes.tweets.item.text\":# or prefix==\"data.item.text\":\n",
    "            if curr_text != '' and curr_tweet!=0:\n",
    "                df.loc[len(df.index)] = [curr_author, curr_tweet, curr_conversation, curr_date, curr_text, curr_scores]\n",
    "            curr_text = value\n",
    "            try:\n",
    "                encoded_input = tokenizer(normalizeTweet(value[0:514])[0:514], return_tensors='pt')\n",
    "                output = model(**encoded_input)\n",
    "                scores = output[0][0].detach().numpy()\n",
    "                # negative, neutral, positive\n",
    "                curr_scores = softmax(scores)\n",
    "            except:\n",
    "                curr_scores = [\"exception\"]\n",
    "        if prefix==\"includes.tweets.item.author_id\":# or prefix==\"data.item.author_id\":\n",
    "            curr_author = value\n",
    "            authors.add(value)\n",
    "        if prefix==\"includes.tweets.item.id\":# or prefix==\"data.item.id\":\n",
    "            #tweetIds.add(value)\n",
    "            tweetCount += 1\n",
    "            curr_tweet = value\n",
    "            tweet2author[value] = curr_author\n",
    "            tweet2text[value] = curr_text\n",
    "        if prefix==\"includes.tweets.item.conversation_id\":# or prefix==\"data.item.conversation_id\":\n",
    "            #tweetIds.add(value)\n",
    "            curr_conversation = value\n",
    "            \"\"\"if value != curr_tweet:\n",
    "                if conversation2tweets.get(value) == None:\n",
    "                    values = []\n",
    "                    values.append(curr_tweet)\n",
    "                    conversation2tweets[value] = values\n",
    "                else:\n",
    "                    values = conversation2tweets.get(value)\n",
    "                    values.append(curr_tweet)\"\"\"\n",
    "        if prefix==\"includes.tweets.item.created_at\":# or prefix==\"data.item.created_at\":\n",
    "            curr_date = dateToTimestampFromTwitter(value)\n",
    "            tweet2date[curr_tweet] = curr_date\n",
    "\n",
    "df.to_json('data/qatar_authorid_sentiment.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# salvataggio delle corrispondenze user_id <-> username per non effettuare altre chiamate per recuperare questa informazione\n",
    "\n",
    "df_users = pd.DataFrame(columns=[\"user_id\", \"username\"])\n",
    "with open(table_qatar, 'rb') as input_file:\n",
    "    parser = ijson.parse(input_file, multiple_values=True)\n",
    "\n",
    "    curr_id = 0   \n",
    "    curr_username = ''\n",
    "    \n",
    "    for prefix, event, value in parser:\n",
    "        if prefix==\"data.item.entities.mentions.item.username\":# or prefix==\"data.item.text\":\n",
    "            if curr_id != 0 and curr_username !='':\n",
    "                df_users.loc[len(df.index)] = [curr_id, curr_username]\n",
    "            curr_username = value\n",
    "        if prefix==\"data.item.entities.mentions.item.id\":# or prefix==\"data.item.text\":\n",
    "            curr_id = value\n",
    "        \n",
    "\n",
    "df_users.to_json('data/df_qatar_authors_username.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/dict_qatar_tweet2author.json', 'w') as f: \n",
    "    json.dump(tweet2author, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generazione degli indici da salvare nel dataset finale e salvataggio corrispondenza user_index <-> user_id\n",
    "count = 0\n",
    "user_idxs = {}\n",
    "for user_id in df['author_id']:\n",
    "    user_idxs[user_id] = count\n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generazione degli indici per i tweet per ricavare l'autore nel dataset finale e salvataggio corrispondenza tweet_index <-> tweet_id\n",
    "cnt = 0\n",
    "tweets_ids = {}\n",
    "for tweet_id in df['tweet_id']:\n",
    "    tweets_ids[tweet_id] = cnt\n",
    "    cnt+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generazione lista dei tweet ricavati per filtrare quelli di cui non si hanno abbastanza informazioni\n",
    "tweets_ids_list = []\n",
    "for tweet_id in df['tweet_id']:\n",
    "    tweets_ids_list.append(tweet_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtraggio\n",
    "df_filtered = df[df['conversation_id'].isin(tweets_ids_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered.to_json('data/qatar_authorid_sentiment_filtered.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author_id</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>conversation_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1526589356747661312</td>\n",
       "      <td>1597882220781268992</td>\n",
       "      <td>1597096711461437440</td>\n",
       "      <td>1.669609e+09</td>\n",
       "      <td>#Arash_Sadeghi is a human rights activist and ...</td>\n",
       "      <td>[0.87306833, 0.12161593, 0.005315802]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1380638725894459397</td>\n",
       "      <td>1597685312876343296</td>\n",
       "      <td>1597882220781268992</td>\n",
       "      <td>1.669796e+09</td>\n",
       "      <td>RT @bettingwithbaz: Australia v Denmark | 30 N...</td>\n",
       "      <td>[0.03936156, 0.8881154, 0.07252303]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1380638725894459397</td>\n",
       "      <td>1597882220047659009</td>\n",
       "      <td>1597685312876343296</td>\n",
       "      <td>1.669749e+09</td>\n",
       "      <td>Australia v Denmark | 30 Nov 2022 | 15:00 \\n(U...</td>\n",
       "      <td>[0.022719333, 0.8554126, 0.121868074]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2388538481</td>\n",
       "      <td>1597722392679559169</td>\n",
       "      <td>1597882220047659009</td>\n",
       "      <td>1.669796e+09</td>\n",
       "      <td>üòÇüòÇüòÇ #Eng #Wales #Qatar2022 https://t.co/ah09qa...</td>\n",
       "      <td>[0.06969667, 0.6644781, 0.26582515]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1303026765737218049</td>\n",
       "      <td>1597882215467474945</td>\n",
       "      <td>1597722392679559169</td>\n",
       "      <td>1.669758e+09</td>\n",
       "      <td>Who made this ü§£ü§£ü§£ü§£ https://t.co/FeHQgCJfod</td>\n",
       "      <td>[0.26182786, 0.63735306, 0.10081909]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262048</th>\n",
       "      <td>252474314</td>\n",
       "      <td>1597705866119192577</td>\n",
       "      <td>1597705866119192577</td>\n",
       "      <td>1.669754e+09</td>\n",
       "      <td>„Éï„É´„Çø„Ç§„É† | „Çª„Éç„Ç¨„É´„Åå„Ç®„ÇØ„Ç¢„Éâ„É´„ÇíÈÄÄ„Åë„ÄÅ„Éé„ÉÉ„ÇØ„Ç¢„Ç¶„Éà„Çπ„ÉÜ„Éº„Ç∏„Å´ÈÄ≤Âá∫ÔºÅüá∏üá≥\\n\\n#Qat...</td>\n",
       "      <td>[0.22035985, 0.74432486, 0.03531532]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262049</th>\n",
       "      <td>2437237330</td>\n",
       "      <td>1597705865406349312</td>\n",
       "      <td>1597705865406349312</td>\n",
       "      <td>1.669754e+09</td>\n",
       "      <td>Sportfotografie √≠s kunst... \\n(Foto Alberto Li...</td>\n",
       "      <td>[0.077792905, 0.8706541, 0.051552974]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262050</th>\n",
       "      <td>828541926701887489</td>\n",
       "      <td>1597666501016027136</td>\n",
       "      <td>1597666501016027136</td>\n",
       "      <td>1.669745e+09</td>\n",
       "      <td>RT @GlVaay: #SayTheirNames \\nHe is Kian. He wa...</td>\n",
       "      <td>[0.8347528, 0.15925573, 0.0059915604]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262051</th>\n",
       "      <td>1037340991898955777</td>\n",
       "      <td>1597705865163059200</td>\n",
       "      <td>1597705865163059200</td>\n",
       "      <td>1.669754e+09</td>\n",
       "      <td>#SayTheirNames \\nHe is Kian. He was only 10 ye...</td>\n",
       "      <td>[0.37334847, 0.5028168, 0.12383477]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262052</th>\n",
       "      <td>1341489526921981954</td>\n",
       "      <td>1597643463683895296</td>\n",
       "      <td>1597643463683895296</td>\n",
       "      <td>1.669739e+09</td>\n",
       "      <td>RT @theidol_: Ghazale Chalabi was an athlete. ...</td>\n",
       "      <td>[0.38994694, 0.557528, 0.05252509]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>260052 rows √ó 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  author_id             tweet_id      conversation_id  \\\n",
       "1       1526589356747661312  1597882220781268992  1597096711461437440   \n",
       "2       1380638725894459397  1597685312876343296  1597882220781268992   \n",
       "3       1380638725894459397  1597882220047659009  1597685312876343296   \n",
       "4                2388538481  1597722392679559169  1597882220047659009   \n",
       "5       1303026765737218049  1597882215467474945  1597722392679559169   \n",
       "...                     ...                  ...                  ...   \n",
       "262048            252474314  1597705866119192577  1597705866119192577   \n",
       "262049           2437237330  1597705865406349312  1597705865406349312   \n",
       "262050   828541926701887489  1597666501016027136  1597666501016027136   \n",
       "262051  1037340991898955777  1597705865163059200  1597705865163059200   \n",
       "262052  1341489526921981954  1597643463683895296  1597643463683895296   \n",
       "\n",
       "           timestamp                                               text  \\\n",
       "1       1.669609e+09  #Arash_Sadeghi is a human rights activist and ...   \n",
       "2       1.669796e+09  RT @bettingwithbaz: Australia v Denmark | 30 N...   \n",
       "3       1.669749e+09  Australia v Denmark | 30 Nov 2022 | 15:00 \\n(U...   \n",
       "4       1.669796e+09  üòÇüòÇüòÇ #Eng #Wales #Qatar2022 https://t.co/ah09qa...   \n",
       "5       1.669758e+09         Who made this ü§£ü§£ü§£ü§£ https://t.co/FeHQgCJfod   \n",
       "...              ...                                                ...   \n",
       "262048  1.669754e+09  „Éï„É´„Çø„Ç§„É† | „Çª„Éç„Ç¨„É´„Åå„Ç®„ÇØ„Ç¢„Éâ„É´„ÇíÈÄÄ„Åë„ÄÅ„Éé„ÉÉ„ÇØ„Ç¢„Ç¶„Éà„Çπ„ÉÜ„Éº„Ç∏„Å´ÈÄ≤Âá∫ÔºÅüá∏üá≥\\n\\n#Qat...   \n",
       "262049  1.669754e+09  Sportfotografie √≠s kunst... \\n(Foto Alberto Li...   \n",
       "262050  1.669745e+09  RT @GlVaay: #SayTheirNames \\nHe is Kian. He wa...   \n",
       "262051  1.669754e+09  #SayTheirNames \\nHe is Kian. He was only 10 ye...   \n",
       "262052  1.669739e+09  RT @theidol_: Ghazale Chalabi was an athlete. ...   \n",
       "\n",
       "                                    sentiment  \n",
       "1       [0.87306833, 0.12161593, 0.005315802]  \n",
       "2         [0.03936156, 0.8881154, 0.07252303]  \n",
       "3       [0.022719333, 0.8554126, 0.121868074]  \n",
       "4         [0.06969667, 0.6644781, 0.26582515]  \n",
       "5        [0.26182786, 0.63735306, 0.10081909]  \n",
       "...                                       ...  \n",
       "262048   [0.22035985, 0.74432486, 0.03531532]  \n",
       "262049  [0.077792905, 0.8706541, 0.051552974]  \n",
       "262050  [0.8347528, 0.15925573, 0.0059915604]  \n",
       "262051    [0.37334847, 0.5028168, 0.12383477]  \n",
       "262052     [0.38994694, 0.557528, 0.05252509]  \n",
       "\n",
       "[260052 rows x 6 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corrispondenza tweet <-> autore\n",
    "tweet2author = {}\n",
    "c = 0\n",
    "for user_id, tweet_id in zip(df_filtered['author_id'], df_filtered['tweet_id']):\n",
    "    tweet2author[tweet_id] = user_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'1597882173088231425'\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# salvataggio del dataset finale contenente id_autore_tweet_risposta, id_autore_tweet_iniziale, timestamp_tweet, testo_tweet\n",
    "df_final = pd.DataFrame(columns=[\"author_id\",  \"tweet_id\", \"conversation_author_id\", \"conversation_id\", \"timestamp\", \"text\", \"sentiment\"])\n",
    "errors = 0\n",
    "for index, row in df_filtered.iterrows():\n",
    "    try:\n",
    "        df_final.loc[len(df_final.index)] = [row['author_id'], row[\"tweet_id\"], tweet2author[row['conversation_id']], row[\"conversation_id\"], row['timestamp'], row['text'].replace('\\n', ' '), row['sentiment']]\n",
    "    except Exception as e:\n",
    "        errors+=1\n",
    "print(str(errors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# salvataggio df per estrazione statistiche\n",
    "df_final.to_csv('data/qatar_df_final_sentiment.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author_id</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>conversation_author_id</th>\n",
       "      <th>conversation_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [author_id, tweet_id, conversation_author_id, conversation_id, timestamp, text, sentiment]\n",
       "Index: []"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The time of execution of above program is : 9402.850866317749 ms\n"
     ]
    }
   ],
   "source": [
    "# ordinamento tweet per timestamp, normalizzazione valori timestamp e salvataggio dataset in csv (con normalizzazione del testo)\n",
    "df_final.sort_values(by='timestamp')\n",
    "df_final['timestamp'] = df_final['timestamp'] - df_final['timestamp'].min()\n",
    "\n",
    "start = time.time()\n",
    "with open('D:\\dataset_input.csv', 'w') as csvfile:\n",
    "    filewriter = csv.writer(csvfile, delimiter=',',\n",
    "                            quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "    filewriter.writerow(['user_id','item_id','timestamp', 'state_label','comma_separated_list_of_features'])\n",
    "    for index, row in df_final.iterrows():\n",
    "        try:\n",
    "            filewriter.writerow([row.user_id, row.item_id, row.timestamp, 0, *getDoc2VecEmbeddingGensim(normalizeTweet(row.text).split(\" \"))])\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "end = time.time()\n",
    "print(\"The time of execution of above program is :\", (end-start) * 10**3, \"ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rimozione righe vuote\n",
    "# rimozione righe vuote\n",
    "with open('D:\\dataset_input.csv') as input, open('tgn/data/dataset_input.csv', 'w', newline='') as output:\n",
    "    writer = csv.writer(output)\n",
    "    for row in csv.reader(input):\n",
    "        if any(field.strip() for field in row):\n",
    "            writer.writerow(row)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SCRAPING PER UTENTE\n",
    "### Per questa fase √® utile memorizzare la corrispondenza indice utente <-> username\n",
    "###### Con le funzioni usate in questo notebook, √® possibile ricavare lo username scorrendo la lista degli user id nel dataframe e controllando la corrispondenza nel dataframe authors_username. √® poi possibile generare di nuovo gli indici e ricavare il corrispondente indice (questo √® un WORKAROUND che si pu√≤ evitare salvando direttamente le corrispondenze indice<->id<->username)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import twint\n",
    "\n",
    "#read df with author_ids\n",
    "df_raw = pd.read_json('df_withauthorid.json')\n",
    "\n",
    "#df with author_id to username correspondance\n",
    "df_users = pd.read_json(\"df_authors_username.json\")\n",
    "#delete duplicates\n",
    "df_users = df_users.drop_duplicates()\n",
    "\n",
    "#get users' indexes used in the final dataframe\n",
    "count = 0\n",
    "user_idxs = {}\n",
    "for user_id in df_raw['author_id']:\n",
    "    user_idxs[user_id] = count\n",
    "    count+=1\n",
    "\n",
    "errors = 0\n",
    "counter = 0\n",
    "start = time.time()\n",
    "\n",
    "for author_id in df_raw['author_id'].drop_duplicates():\n",
    "    try:\n",
    "        # estrazione username\n",
    "        username = df_users[df_users['user_id']==author_id].drop_duplicates()['username'].values[0]\n",
    "        # creazione oggetto twint per lo scraping\n",
    "        c = twint.Config()\n",
    "        c.Username = username\n",
    "        c.Limit = 100 # estrazione di 100 tweet per utente\n",
    "        c.Output = \"tweets/tweets-\"+str(user_idxs[author_id])+\".json\" # salvataggio dei tweet in un file json con nome contenente l'indice dell'utente\n",
    "        c.Store_json = True\n",
    "        twint.run.Search(c)\n",
    "        counter+=1\n",
    "        print(username + \"saved\") # log degli utenti salvati\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        errors +=1\n",
    "\n",
    "print(\"Time elapsed: \"+ str(time.time() - start))\n",
    "print(\"Saved:\" + str(counter))\n",
    "print(\"Errors:\" + str(errors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading completed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_idx</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100092</td>\n",
       "      <td>üá™üá®‚ùåÔ∏è‚öΩÔ∏è ¬°ECUADOR NO CONOCE LA VICTORIA EN EL H...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100419</td>\n",
       "      <td>@VoltInuOfficial $VOLT ü•áüåç‚ö°‚ö°‚ö°‚ö°‚ö°‚ö°‚ö°‚ö°  https://t....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100511</td>\n",
       "      <td>@aghachurchill @unrulified ÿ≥ŸÑÿßŸÖ ÿÆ€åŸÑ€å ÿÆŸàÿ® ⁄©ÿßÿ± ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10065</td>\n",
       "      <td>Eat it‚Ä¶ Feel it‚Ä¶ Love it‚Ä¶  Celebrate everyday...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100709</td>\n",
       "      <td>¬°Lindo inicio de semana para todos!   #FelizL...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3102</th>\n",
       "      <td>99833</td>\n",
       "      <td>U.S. charges neo-Nazi leader over plot to att...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3103</th>\n",
       "      <td>99834</td>\n",
       "      <td>#LunesDeGanarSeguidores #mexicana #sexy  http...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3104</th>\n",
       "      <td>9991</td>\n",
       "      <td>Your gauge determines your week!  Choose wise...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3105</th>\n",
       "      <td>99938</td>\n",
       "      <td>30 anggota @InterClubIndo merayakan kemenanga...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3106</th>\n",
       "      <td>99965</td>\n",
       "      <td>‡Æö‡Æ∞‡Øç‡Æµ‡Æ§‡Øá‡Æö ‡Æï‡Æø‡Æ∞‡Æø‡Æï‡Øç‡Æï‡ØÜ‡Æü‡Øç ‡Æ™‡Øã‡Æü‡Øç‡Æü‡Æø‡Æï‡Æ≥‡Æø‡Æ≤‡Øç ‡Æá‡Æ∞‡ØÅ‡Æ®‡Øç‡Æ§‡ØÅ ‡Æì‡ÆØ‡Øç‡Æµ‡ØÅ ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3107 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     user_idx                                               text\n",
       "0      100092   üá™üá®‚ùåÔ∏è‚öΩÔ∏è ¬°ECUADOR NO CONOCE LA VICTORIA EN EL H...\n",
       "1      100419   @VoltInuOfficial $VOLT ü•áüåç‚ö°‚ö°‚ö°‚ö°‚ö°‚ö°‚ö°‚ö°  https://t....\n",
       "2      100511   @aghachurchill @unrulified ÿ≥ŸÑÿßŸÖ ÿÆ€åŸÑ€å ÿÆŸàÿ® ⁄©ÿßÿ± ...\n",
       "3       10065   Eat it‚Ä¶ Feel it‚Ä¶ Love it‚Ä¶  Celebrate everyday...\n",
       "4      100709   ¬°Lindo inicio de semana para todos!   #FelizL...\n",
       "...       ...                                                ...\n",
       "3102    99833   U.S. charges neo-Nazi leader over plot to att...\n",
       "3103    99834   #LunesDeGanarSeguidores #mexicana #sexy  http...\n",
       "3104     9991   Your gauge determines your week!  Choose wise...\n",
       "3105    99938   30 anggota @InterClubIndo merayakan kemenanga...\n",
       "3106    99965   ‡Æö‡Æ∞‡Øç‡Æµ‡Æ§‡Øá‡Æö ‡Æï‡Æø‡Æ∞‡Æø‡Æï‡Øç‡Æï‡ØÜ‡Æü‡Øç ‡Æ™‡Øã‡Æü‡Øç‡Æü‡Æø‡Æï‡Æ≥‡Æø‡Æ≤‡Øç ‡Æá‡Æ∞‡ØÅ‡Æ®‡Øç‡Æ§‡ØÅ ‡Æì‡ÆØ‡Øç‡Æµ‡ØÅ ...\n",
       "\n",
       "[3107 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "count=0\n",
    "df_user2text = pd.DataFrame(columns=('user_idx', 'text'))\n",
    "files = glob.glob('tweets/*', recursive=True)\n",
    "# per ogni utente, i tweet vengono concatenati e salvati in un dataframe\n",
    "for single_file in files:\n",
    "    with open(single_file, 'r') as f:\n",
    "        user_idx = str(single_file).split('-')[1].split('.')[0]\n",
    "        parser = ijson.parse(single_file, multiple_values=True, )      \n",
    "        tweet = ''\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            tweet = tweet + ' ' + data[u'tweet']\n",
    "        df_user2text.loc[len(df_user2text)] = [user_idx, tweet]\n",
    "        \n",
    "# per ogni utente si genera l'embedding del testo complessivo e si salva nel dataset, assieme all'indice dell'utente\n",
    "with open('D:\\dataset_utenti.csv', 'w') as csvfile:\n",
    "    filewriter = csv.writer(csvfile, delimiter=',',\n",
    "                            quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "    filewriter.writerow(['user_id', 'comma_separated_list_of_features'])\n",
    "    for index, row in df_user2text.iterrows():\n",
    "        try:\n",
    "            # √® necessario che ogni vettore di embedding sia composto da 172 valori (√® necessario allenare un nuovo modello doc2vec: model = gensim.models.doc2vec.Doc2Vec(vector_size=172, min_count=2, epochs=30) )\n",
    "            filewriter.writerow([row.user_idx, *getDoc2VecEmbeddingGensim(normalizeTweet(row.text).split(\" \"))])\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rimozione righe vuote e salvataggio dataset nel path data (il nome deve essere '{dataName}_nodes.csv')\n",
    "with open('D:\\dataset_utenti.csv') as input, open('tgn/data/dataset_input_nodes.csv', 'w', newline='') as output:\n",
    "    writer = csv.writer(output)\n",
    "    for row in csv.reader(input):\n",
    "        if any(field.strip() for field in row):\n",
    "            writer.writerow(row)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistiche sui dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('df_final.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stats = pd.DataFrame(columns=[\"user_id\", \"interactions\"])\n",
    "df_stats_asCreator = pd.DataFrame(columns=[\"user_id\", \"interactions\"])\n",
    "df_stats_asCreator['user_id'] = df[df['user_id']!=df['item_id']]['user_id'].value_counts().index\n",
    "df_stats_asCreator['interactions'] = df[df['user_id']!=df['item_id']]['user_id'].value_counts().values\n",
    "df_stats_asReplyer = pd.DataFrame(columns=[\"user_id\", \"interactions\"])\n",
    "df_stats_asReplyer['user_id'] = df[df['user_id']!=df['item_id']]['item_id'].value_counts().index\n",
    "df_stats_asReplyer['interactions'] = df[df['user_id']!=df['item_id']]['item_id'].value_counts().values\n",
    "\n",
    "df_stats = pd.concat([df_stats_asCreator, df_stats_asReplyer])\\\n",
    "       .groupby('user_id')['interactions']\\\n",
    "       .sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_stats['interactions'].plot(kind='kde', color=\"darkblue\", xlabel=\"#interactions\",figsize=(20,5))\n",
    "plt.grid(color='blue', linestyle='-', linewidth=0.5, axis=\"both\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Statistiche numero di interazioni')\n",
    "print('Minimo: ' + str(df_stats['interactions'].min()))\n",
    "print('Moda: ' + str(df_stats['interactions'].mode()))\n",
    "print('Mediana: ' + str(df_stats['interactions'].median()))\n",
    "print('Media: ' + str(df_stats['interactions'].mean()))\n",
    "print('Massimo: ' + str(df_stats['interactions'].max()))\n",
    "print('Totale: ' + str(df_stats['interactions'].sum()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6e507f021528ee598d4189b61a0c3eda1bdaecd993998d2871a13eaf733316a7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
